{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S2-NNF-DS10",
      "language": "python",
      "name": "u4-s2-nnf-ds10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "ds-cs-N431a_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10OXb-oNKVzO"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 3 / Assignment 1*\n",
        "# Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model 을 사용해보기\n",
        "- <a href=\"#p2\">Part 2:</a> Custom CNN Model을 제작해보기\n",
        "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
        "\n",
        "\n",
        "케라스를 이용한 바이너리 이미지 분류 모델에 3가지 CNN 모델을 적용한다.  <br>\n",
        "[데이터 다운로드](https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/mountainForest.zip) <br>\n",
        "산의 이미지(./data/mountin/*)와 숲의 이미지(./data/forest/*)를 분류하십시오. 산을 포스티브 클래스(1)로, 숲 이미지를 네거티브(0)로 처리한다.\n",
        "\n",
        "|Mountain (+)|Forest (-)|\n",
        "|---|---|\n",
        "|![](https://github.com/codestates/ds-cs-section4-sprint3/blob/main/N431/data/mountain/art1131.jpg?raw=1)|![](https://github.com/codestates/ds-cs-section4-sprint3/blob/main/N431/data/forest/cdmc317.jpg?raw=1)|\n",
        "\n",
        "표본이 작다는 점을 감안하면 문제는 현실적으로 어렵다. 즉, 클래스당 약 350개의 관측치가 있다. 이 샘플 크기는 직장에서 이미지 분류 문제/솔루션을 프로토타이핑할 때 기대할 수 있는 것일 수 있다. 이럴 때에는 여러가지 모델을 적용해보는 것이 중요하기 때문에 이번 sprint에서는 가능한 여러 모델을 평가하는 데 익숙해지는 것이 중요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMOEbZu9wjFD"
      },
      "source": [
        "### 문항 1) CNN을 설명한 문장에서 틀린 것을 모두 고르시오. \n",
        "1. CNN은 이미지 처리에 탁월한 성능을 보이는 신경망이다. \n",
        "2. CNN의 핵심 모듈은 합성곱(convolution)과 pooling으로 기존신경망에서 사용하지 않던 개념이 새롭게 도입되었다. \n",
        "3. Weight sharing 개념이 도입되었다.\n",
        "4. 이미지의 RGB처럼 3개의 채널의 정보를 이용할 수 있고, 각각의 채널에서 가중치를 곱하여 특징의 숫자를 늘리는 등 채널을 변형하여 더 많은 채널로 특징을 추출한다.  \n",
        "5. 슬라이딩 윈도우 방식으로 이미지에 filter들을 곱하는 형태로 적용한다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGMReRokSXbN"
      },
      "source": [
        "### 문항 2) CNN을 설명한 문장에서 틀린 것을 모두 고르시오. \n",
        "6. 합성곱연산을 통해 나온 결과를 특성 맵(Feature map)이라 한다. \n",
        "7. filter의 개수와 맞춰서 연산을하고, 합성곱 이후에도 이미지의 사이즈를 유지하려고 패딩(padding)이란 기술을 사용한다. \n",
        "8. 가중치와 편향의 계산법은 일반 신경망과 동일하다.\n",
        "9. stride가 1보다 크면 합성곱을 통과하더라도 이미지의 사이즈가 일정하다. \n",
        "10. 3차원 convolution도 수행할 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Ng0p_35yrh"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI3qR2nVKVzT"
      },
      "source": [
        "## Pre - Trained Model\n",
        "<a id=\"p1\"></a>\n",
        "\n",
        "Keras에서 제공하는 pre-trained 모델인 ResNet50을 불러오고, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - 이 모델은 50 개의 layer를 가진  CNN기반의 모델입니다. 총 [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt)를 분류하는 모델로 아래와 같이 사용할 수 있습니다. 우리가 풀어야 할 과제는 2가지 이니, 마지막 출력 단을 변경해서 사용하면 됩니다.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D()\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "```\n",
        "\n",
        "`ResNet50`을 불러올 때, `include_top`을 False로 하면, 기존 1000개의 분류 문제를 풀 수 있는 ResNet 모델에서 Fully Connected layer를 제거해주는 역할을 합니다. \n",
        "\n",
        "```python\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "```\n",
        "이 부분은 ResNet50 레이어들의 파라미터 학습모드를 끄는(off) 것입니다. 이렇게 설정된 매개 변수는 설령 지금 순전파/역전파 이후 오류가 전파 되더라도 파라미터가 학습(업데이트)되지 않는 것입니다. \n",
        "\n",
        "Keras 기능 API를 사용하여 모델에 추가로 `Fully-conneted layer`(Dense모델)을 추가해야합니다. 우리는 최상위 레이어,  `Fully-conneted layer`를 제거합니다. 출력층의 숫자(=분류할 클래스의 개수)가 맞지 않기 때문이죠. 네트워크의 특징을 추출하는 부분 만 유지하는 것입니다. `GlobalAveragePooling2D` 레이어는 마지막 컨벌루션 레이어 출력 (2 차원) 각각의 평균을 취함으로써 정말 멋진 평탄화 기능으로 작동합니다.\n",
        "\n",
        "```python\n",
        "x = res.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x) # 출력층 설계\n",
        "model = Model(res.input, predictions)\n",
        "```\n",
        "\n",
        "이 과제는 산의 이미지 (`. / data / mountain / *`)와 숲의 이미지 (`. / data / forest / *`)를 분류하기 위해 위의 전이 학습(transfer learning)을 적용하는 것입니다. 산을 postive 클래스 (1)로, 숲 이미지를 음수 (0)로 취급합니다.\n",
        "\n",
        "Steps to complete assignment: \n",
        "1. 이미지 데이터를 numpy 배열 (`X`)로 불러옵니다.\n",
        "2. 레이블에 대한 `y`를 만듭니다.\n",
        "3. resnet의 사전 훈련 된(pre-trained) 레이어로 모델 훈련\n",
        "4. 모델의 정확성보고"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0vk7Dumyowj"
      },
      "source": [
        "### 문항 3) Cifar100같이 100개의 클래스로 Transfer learning을 해야 한다면, 아래 코드를 어떻게 바꿔야 하는가? <br> 변경된 코드를 입력하시오. \n",
        "```\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE6qTpqGSdYq"
      },
      "source": [
        "\n",
        "### 문항 4) 전이학습을 진행할 때, `layer.trainable = True`로 진행하는 것과 'False'일 때의 차이에 대해서 틀린 설명을 **모두** 고르시오.\n",
        "\n",
        "- [1] True인 경우 네트워크에 있는 모든 파라미터들이 학습이 진행되어, 학습속도가 느려진다. \n",
        "- [2] True인 경우에도 학습속도가 느려지진 않는다.\n",
        "- [3] False인 경우가 True인 경우보다 학습할 파라미터의 개수가 적다. \n",
        "- [4] False인 경우와 True인 경우 총 가중치 파라미터의 개수는 같다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvJd3ItAKVzU"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "![skimage-logo](https://scikit-image.org/_static/img/logo.png)\n",
        "\n",
        "기존 학습된 네트워크(모델)을 사용하려면 기존 입력값을 동일하게 맞춰주어야 합니다.  [`skimage`](https://scikit-image.org/) 의 문서를 확인하시면 보다 쉽게 사용하실 수 있을 것입니다. 다음 함수를 사용해보세요. `skimage.io.imread_collection` 와 `skimage.transform.resize`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqKzhPeZ0mPE"
      },
      "source": [
        "### 문항 5) 이미지 입력을 resize 해야하는 이유와 함수를 사용하는 방법에 대해서 틀린 것을 **모두** 고르시오. 이를 해결하기 위해서 실제 구동을 시켜보아도 된다. \n",
        "- [1] 다른 사이즈의 이미지를 입력하게 되면, 연산이 되지 않는다. \n",
        "- [2] 다른 사이즈의 이미지를 입력하게 되면 기존의 파라미터가 특징을 추출하는데 적절한 정보가 담기지 못한다. \n",
        "- [3] resize함수를 default로 이용하면, 기존 이미지를 interpolation하여 이미지를 생성한다. \n",
        "- [4] resize를 사용할 때 선택할 수 있는 모드의 개수는 총 5개이다. \n",
        "- [5] mode에 따라서 인터폴레이션이 되기도, 안 되기도 한다. \n",
        "- [6] anti_aliasing 을 이용하면 가우시안 필터를 입히는 데, 이는 다운샘플링을 할 때 artifact를 제거하는데 효과적이다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4c0M58cKVzW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMzezBMBKVza"
      },
      "source": [
        "## Instatiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0i14sSCKVzb",
        "outputId": "74a33d44-bf1f-4542-8339-f02985a1b731"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/jonathansokoll/anaconda3/envs/U4-S3-DNN/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtQX3ALdKVzf"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPEt-wwuKVzg",
        "outputId": "1545ee98-10d5-4253-d11e-13effb509682"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 561 samples, validate on 141 samples\n",
            "Epoch 1/5\n",
            "561/561 [==============================] - 39s 69ms/sample - loss: 0.1216 - accuracy: 0.9715 - val_loss: 1.4221 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "561/561 [==============================] - 35s 62ms/sample - loss: 0.0410 - accuracy: 0.9875 - val_loss: 2.9222 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "561/561 [==============================] - 35s 62ms/sample - loss: 0.0646 - accuracy: 0.9697 - val_loss: 0.9962 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "561/561 [==============================] - 39s 70ms/sample - loss: 0.0723 - accuracy: 0.9733 - val_loss: 0.2627 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "561/561 [==============================] - 38s 68ms/sample - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.4859 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f980e9c6f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBDRtCw55Sb"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpEjsFpKVzj"
      },
      "source": [
        "## Custom CNN Model\n",
        "\n",
        "\n",
        "이 단계에서는 Keras를 사용하여 자신 만의 CNN을 작성하고 훈련합니다. 네트워크 시작 부분에 적어도 하나의 Conv 레이어와 pooling 레이어가있는 아키텍처를 사용할 수 있습니다. 원하는 경우 더 추가 해볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgwLFkhUKVzj",
        "outputId": "5f9184eb-4a1d-4f2e-a83e-ad6bdf063a71"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 215, 215, 32)      9632      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 43, 43, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 39, 39, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 9, 9, 64)          102464    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5184)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 64)                331840    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 495,265\n",
            "Trainable params: 495,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5NrX3K9KVzm"
      },
      "source": [
        "# Compile Model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yju2nvVNKVzp",
        "outputId": "0cd03a87-2b28-4ade-a721-086627273a94"
      },
      "source": [
        "# Fit Model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 561 samples, validate on 141 samples\n",
            "Epoch 1/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.2667 - accuracy: 0.9073 - val_loss: 0.1186 - val_accuracy: 0.9858\n",
            "Epoch 2/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.2046 - accuracy: 0.9073 - val_loss: 0.3342 - val_accuracy: 0.8511\n",
            "Epoch 3/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.1778 - accuracy: 0.9287 - val_loss: 0.2746 - val_accuracy: 0.8723\n",
            "Epoch 4/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.1681 - accuracy: 0.9323 - val_loss: 0.8487 - val_accuracy: 0.5957\n",
            "Epoch 5/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.1606 - accuracy: 0.9394 - val_loss: 0.3903 - val_accuracy: 0.8582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97388777f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgrfVsI55_z6"
      },
      "source": [
        "# Part 3. 도전과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt1dVfOlKVzs"
      },
      "source": [
        "## 도전과제1. Image Manipulations(Augmentation)을 이용하여 성능을 개선해 보시오. \n",
        "\n",
        "이미지 샘플의 증가를 시뮬레이션하기 위해 자르기, 회전, 늘이기 등과 같은 이미지 조작 기술을 적용 할 수 있습니다. 운 좋게도 Keras에는 이러한 기술을 산과 숲 예제에 적용 할 수있는 몇 가지 편리한 기능이 있습니다. 시작하는 데 도움이되는 다음 리소스를 확인하세요.\n",
        "\n",
        "참고자료 \n",
        "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
        "3. [Data Augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## 도전과제 2. 모델 개선하기\n",
        "- 코드 향상시키기 : 지난 주 강의 자료를 참조하여 클래스와 내부 method를 이용하여 동작하고, 직접 다운로드 한 이미지를 인식시켜 범주(Class)를 분류하도록 코드개선합니다 (예 : 비행기, 풍선 이미지 다운로드 및 인식되는 지 확인)\n",
        "- 데이터 augmentation을 이용하여 학습 및 분류 진행해보기 (단 Training Set, Test Set의 데이터가 섞이지 않도록 주의 필요)\n",
        "- [기타 사용 가능한 사전 훈련 된 네트워크](https://tfhub.dev)를 확인하고 몇 가지 시도하고 비교해보세요.\n",
        "- 전이 학습을 이용하되, 직접 가져온 이미지를 넣고 새로운 카테고리로 분류해보세요. [Classifier 재학습](https://www.tensorflow.org/hub/tutorials/image_retraining)\n",
        "\n",
        "Resources\n",
        "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
        "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
        "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
        "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
        "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
      ]
    }
  ]
}