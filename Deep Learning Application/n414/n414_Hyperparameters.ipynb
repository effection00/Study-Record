{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "ds-cs-N424-Lecture.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41TS0Sa0rDNx"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 4*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWd0b3FNfdw4"
      },
      "source": [
        "# 신경망과 학습에 관련된 파라미터 튜닝\n",
        "*aka Hyperparameter Tuning*\n",
        "\n",
        "*aka Big Servers for Big Problems*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-uha0MZi3np"
      },
      "source": [
        "WarmUp\n",
        "- [하이퍼파라미터 튜닝이 뭐지?](https://www.youtube.com/watch?v=wKkcBPp3F1Y) (재방)\n",
        "- 학습 규제 방식\n",
        "  * [L1/L2-regularization](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c), DropOut, Gradient Vanishing, Xavier 초기값\n",
        "  <img src=\"https://i.stack.imgur.com/oVJDB.png\">\n",
        "- [Gradient Descent With Momentum](https://youtu.be/yWQZcdJ4k8s?t=34)\n",
        "  * 학습해왔던 관성의 법칙을 유지하는 방식으로 학습 개선\n",
        "- [Batch Size](https://youtu.be/U4WB9p6ODjM?t=29)\n",
        "  * Batch를 크게하면 좋은 이유\n",
        "  * 그러나 항상 크게할 수 없는 이유\n",
        "  * 일반적으로 Batch라고 하면 Mini-batch를 의미한다는 점\n",
        "- [하이퍼파라미터 튜닝은 왜 해야 하는가?](https://www.youtube.com/watch?v=-i8b-srMhGM)\n",
        "- 강의자료 맨 아래, \"실험 기록 프레임워크\"의 Wandb [QuickStart](https://docs.wandb.com/quickstart)를 보고 회원가입 등을 해두세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UDOYvSDnveA"
      },
      "source": [
        "# 지난시간 복습\n",
        "- 신경망의 순전파와 역전파 (Note 1-2)\n",
        "  * 신경망의 순전파 (Note 1) - $\\Sigma (Node_j$ x $Edge_{ij}) + bias $\n",
        "  * 신경망의 역전파 (Note 2) - $\\partial E \\over \\partial W_i$, Chain Rule\n",
        "  * 모델 생성과 모델 초기화 (Note 3)\n",
        "  * 경사하강법의 다양성 (Note 2-3)\n",
        "  * 학습 과정에서 알아야 할 Tricks (Note 3) - Weight Decay/Constraint, Dropout, Learning Rate\n",
        "\n",
        "- 그간 다뤄본 데이터\n",
        "  * MNIST\n",
        "  * Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yafv0VXfdw5"
      },
      "source": [
        "# 🏆 학습목표\n",
        "* <a href=\"#p1\">Part 1</a>: 대표적인 하이퍼 파라미터를 설명 할 수 있습니다\n",
        "* <a href=\"#p2\">Part 2</a>: ETF (Experiment Tracking Framework)에 대해 알아보고 적용할 수 있습니다.\n",
        "* <a href=\"#p3\">Part 3</a>: (Optional) RandomSearch를 사용해서 하이퍼 파라미터 공간에서 최적의 하이퍼 파라미터를 찾을 수 있습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEAIyEefdw6"
      },
      "source": [
        "# Hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjDZrxWdfdw6"
      },
      "source": [
        "## 파라미터 튜닝으로 성능 올리기\n",
        "\n",
        "  하이퍼 파라미터 (Hyperparameter) 튜닝은 우리가 다뤄왔었던 다른 머신러닝 알고리즘들보다 신경망에서 훨씬 더 중요합니다. 다른 지도학습 (supervised learning) 알고리즘에는 소수의 몇몇 파라미터 정도만 손보면 됐었지만, 신경망에는 훨씬 더 많은 파라미터들을 조정해야합니다. 이 파라미터들은 모델의 정확도에 엄청난 영향을 끼치기 때문에 시간은 많이 소요되더라도 신경망을 다룬다면 반드시 거쳐야하는 단계입니다. 중요한 하이퍼파라미터들은 일전 강의에서도 몇번 다뤘습니다. 오늘은 조금 더 포괄적인 개념으로 접근해보겠습니다. \n",
        "\n",
        "  먼저 아셔야 할 것은 ​신경망을 위한 하이퍼 파라미터 튜닝은 쉽지않습니다. '노가다'라고 말했던 것을 기억하시나요? 만약 모델의 최종 에러 평가지표(error metric)가 Linear하지 않고 불규칙적으로 변한다면 서로 다른 하이퍼 파라미터로 지정된 모델을 어떻게 비교할 수 있을까요? \n",
        "  \n",
        "  운 좋게 좋은 파라미터들이 걸릴 수도 있겠지만, 운이 안 좋아서 틀린 하이퍼 파라미터를 선택하게 되는 것을 어떻게 피할 수 있을까요? 사실 예제로 명시된 두 문제는 어떤 면에선 우리가 실험을 계속한다면 안고가야할 문제입니다. 실험들을 교차 검증(Cross Validation)한다면 최종적으로 계산되는 정확도들의 분산(variance)을 줄일 수 있어서 이러한 문제점들을 최소화가 가능합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s0o2pqBs88q"
      },
      "source": [
        "실습 - Boston_Housing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKpOqNHXfdw7",
        "outputId": "8728cb5e-9ba2-4322-a82f-8271a2d61ff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 데이터를 불러옵니다. \n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKqVyTbmBMDe"
      },
      "source": [
        "## 도전과제 CV?? 코드를 어떻게 시작할까?\n",
        "\n",
        "에러를 만나면 하나씩 해결하는 코드를 포함하여 튜토리얼을 진행합니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtkZXWIqCVOB"
      },
      "source": [
        "Google에 검색 : keras crossvalidation\n",
        "\n",
        "[검색 첫 페이지]() - 꽝 <Br>\n",
        "[검색 두번째 페이지](https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_4FOesVn0qD",
        "outputId": "3460bc01-bd19-425b-abd6-af4c58b6d44c"
      },
      "source": [
        "print(x_train[:2])\n",
        "print(y_train[:2])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.23247e+00 0.00000e+00 8.14000e+00 0.00000e+00 5.38000e-01 6.14200e+00\n",
            "  9.17000e+01 3.97690e+00 4.00000e+00 3.07000e+02 2.10000e+01 3.96900e+02\n",
            "  1.87200e+01]\n",
            " [2.17700e-02 8.25000e+01 2.03000e+00 0.00000e+00 4.15000e-01 7.61000e+00\n",
            "  1.57000e+01 6.27000e+00 2.00000e+00 3.48000e+02 1.47000e+01 3.95380e+02\n",
            "  3.11000e+00]]\n",
            "[15.2 42.3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUzqr9TioyAK",
        "outputId": "c16af499-016c-4c21-9e64-e9394a4c4b8f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "kf = KFold(n_splits = 5)             \n",
        "skf = StratifiedKFold(n_splits = 5, random_state = 100, shuffle = True) \n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pov3LsNeqN5a",
        "outputId": "dd6b7e3b-bb70-453e-fc02-d87be451ed2c"
      },
      "source": [
        "y_train[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15.2, 42.3, 50. , 21.1, 17.7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBXqRxk9VD3Y"
      },
      "source": [
        "아래부터는 에러가 나올 것입니다. 아래 설명을 충분히 읽으면서 실행하세요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "h_U6o1E_pNul",
        "outputId": "f7c20680-8e85-4e8c-cb31-5c68dd73bdf3"
      },
      "source": [
        "training_data = x_train.iloc[train_index]\n",
        "validation_data = x_train.iloc[val_index]\n",
        "\n",
        "# for train_index, val_index in kf.split(np.zeros(x_train.shape[0]),y_train):\n",
        "#   training_data = x_train.iloc[train_index]\n",
        "#   validation_data = x_train.iloc[val_index]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-53c6661f6ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for train_index, val_index in kf.split(np.zeros(x_train.shape[0]),y_train):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   training_data = x_train.iloc[train_index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ2Yaz4s4RUS"
      },
      "source": [
        "Numpy 어레이의 경우는 iloc을 쓸 수 없겠죠? 그러니 바꿔줍니다. pd.DataFrame()을 이용합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUc--NhcqYH5"
      },
      "source": [
        "x_train = pd.DataFrame(x_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "for train_index, val_index in kf.split(np.zeros(x_train.shape[0]), y_train):\n",
        "  training_data = x_train.iloc[train_index]\n",
        "  validation_data = x_train.iloc[val_index]\n",
        "  training_y = y_train.iloc[train_index]\n",
        "  validation_y = y_train.iloc[val_index]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m66IH-APZJQ"
      },
      "source": [
        "# 데이터 확인\n",
        "# training_data, validation_data, training_y, validation_y\n",
        "validation_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj9YID8t4YZ5"
      },
      "source": [
        "모델을 불러오는데 에러가 납니다! 왜 일까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "Nj7jdAVsqiqi",
        "outputId": "1eedc653-636a-4379-956b-05a4bd2dd1dd"
      },
      "source": [
        "# CREATE NEW MODEL\n",
        "model = Sequential()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-28c7a5557c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CREATE NEW MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ZKtiJl4bcd"
      },
      "source": [
        "너무 쉽습니다. defined 되지 않았다고 나오는 것을 보고, 모델을 import해주면 해결됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFL4K6_Kqwdh",
        "outputId": "e5588b44-a118-44d8-d415-a35a4ee259c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "# CREATE NEW MODEL\n",
        "model = Sequential()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8e0343ba6c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# CREATE NEW MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dense' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jJEAa0oVeGx"
      },
      "source": [
        "이번에는 Dense를 추가해줍니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toZUFld9VckA",
        "outputId": "7b32e576-cea1-4548-cd5a-0c7fa70f6dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8a2b07a9a70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dense' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hhrnRrkVhlU"
      },
      "source": [
        "같은 유형의 에러는 똑같이 잡아줍니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BJ-QZU3rJCT"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "# CREATE NEW MODEL\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# COMPILE NEW MODEL\n",
        "model.compile(loss='mean_squared_logarithmic_error',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf3kVPVQrbMA",
        "outputId": "906e593a-c12e-434b-8251-0e487132c7f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(training_data, training_y,\n",
        "\t\t\t    epochs=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "11/11 [==============================] - 3s 7ms/step - loss: 6.4284 - accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2.0370 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f27a06eb310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAcwpMHx4mZ-"
      },
      "source": [
        "모델이 돌아가는 것까지 확인했죠? 이제는 CV를 해야하니까 다시 train을 각각의 set으로 나눌 수 있도록 조치를 해주어야 합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpxr_uW3sRPh"
      },
      "source": [
        "x_train = pd.DataFrame(x_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "for train_index, val_index in kf.split(np.zeros(x_train.shape[0]),y_train):\n",
        "  training_data = x_train.iloc[train_index, :]\n",
        "  training_data_label = y_train.iloc[train_index]\n",
        "  validation_data = x_train.iloc[val_index, :]\n",
        "  validation_data_label = y_train.iloc[val_index]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1OYr2XQsWv5",
        "outputId": "e41c311d-86d9-4458-d75d-046e7b4febcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(training_data, training_data_label,\n",
        "\t\t\t    epochs=10,\n",
        "          batch_size=64,\n",
        "\t\t\t    validation_data=(validation_data, validation_data_label),\n",
        "          )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 1.8608 - accuracy: 0.0000e+00 - val_loss: 1.1074 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.9953 - accuracy: 0.0000e+00 - val_loss: 0.2010 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4042 - accuracy: 0.0000e+00 - val_loss: 0.1352 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2885 - accuracy: 0.0000e+00 - val_loss: 0.2758 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2826 - accuracy: 0.0000e+00 - val_loss: 0.0874 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1584 - accuracy: 0.0000e+00 - val_loss: 0.0911 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.0000e+00 - val_loss: 0.1035 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.0000e+00 - val_loss: 0.1644 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1224 - accuracy: 0.0000e+00 - val_loss: 0.0833 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1030 - accuracy: 0.0000e+00 - val_loss: 0.0762 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f27a044e1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmI97_mg458S"
      },
      "source": [
        "데이터가 잘 나누어져 들어갔는 지 확인을 해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t0RcWrl9Dpy",
        "outputId": "5112f064-d973-456e-f7e1-6f78bf688d12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(training_data[:2])\n",
        "print(training_data.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        0     1     2    3      4   ...   8      9     10      11     12\n",
            "0  1.23247   0.0  8.14  0.0  0.538  ...  4.0  307.0  21.0  396.90  18.72\n",
            "1  0.02177  82.5  2.03  0.0  0.415  ...  2.0  348.0  14.7  395.38   3.11\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "(324, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfDc4XEv9Hls",
        "outputId": "1fa5b648-5512-440f-f558-c10be7426e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "training_data_label[:2]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0\n",
              "0  15.2\n",
              "1  42.3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjVIK2C5tNf9"
      },
      "source": [
        "# COMPILE NEW MODEL\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(training_data, training_data_label,\n",
        "\t\t\t    epochs=10,\n",
        "          batch_size=30,\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M--JTnHttNiS"
      },
      "source": [
        "# COMPILE NEW MODEL\n",
        "# 다양한 loss로 테스트도 해봅니다. \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam') # binary_crossentropy # mean_squared_error\n",
        "model.fit(x_train, y_train,\n",
        "\t\t\t    epochs=10,\n",
        "          batch_size=30,\n",
        "          validation_data = (validation_data, validation_data_label),\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IkKU8AeAebU",
        "outputId": "262935b7-7ae3-4501-d5de-65164e15bb01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test mse:\", results)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1035 - accuracy: 0.0000e+00\n",
            "test loss, test mse: [0.10349596291780472, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fpbmwH1V5os"
      },
      "source": [
        "이제 한번에 테스트를 수행해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mllc_oM_tNky",
        "outputId": "f1a6c71e-9335-4b9c-b60d-76f2da6fac1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_train = pd.DataFrame(x_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "for train_index, val_index in kf.split(np.zeros(x_train.shape[0])):\n",
        "  training_data = x_train.iloc[train_index, :]\n",
        "  training_data_label = y_train.iloc[train_index]\n",
        "  validation_data = x_train.iloc[val_index, :]\n",
        "  validation_data_label = y_train.iloc[val_index]\n",
        "\n",
        "  # CV\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  model.fit(x_train, y_train,\n",
        "\t\t\t    epochs=10,\n",
        "          batch_size=30,\n",
        "          validation_data = (validation_data, validation_data_label),\n",
        "          )\n",
        "  results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "  print(\"test loss, test mse:\", results)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 102.9852 - val_loss: 93.7256\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 67.2748 - val_loss: 57.7358\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 49.8223 - val_loss: 55.9194\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 49.8231 - val_loss: 39.7117\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 47.2116 - val_loss: 42.8538\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 45.9381 - val_loss: 37.8990\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 42.8675 - val_loss: 42.8356\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 47.5727 - val_loss: 44.0944\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 43.8427 - val_loss: 40.6398\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 41.9551 - val_loss: 40.5802\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 47.8371\n",
            "test loss, test mse: 47.837059020996094\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 82.7760 - val_loss: 43.2994\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 45.9407 - val_loss: 40.3576\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 39.0271 - val_loss: 34.4319\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 38.8412 - val_loss: 36.4533\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 42.5803 - val_loss: 53.7748\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 47.6793 - val_loss: 49.2857\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 38.4568 - val_loss: 32.0501\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 36.7490 - val_loss: 31.5892\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 37.0933 - val_loss: 34.4987\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 37.7142 - val_loss: 38.5857\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 42.7149\n",
            "test loss, test mse: 42.714935302734375\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 83.3224 - val_loss: 45.6503\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 61.3771 - val_loss: 35.9423\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 44.3376 - val_loss: 21.6455\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 40.5216 - val_loss: 37.5071\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 37.2096 - val_loss: 20.3609\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 35.4577 - val_loss: 21.9378\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 35.2711 - val_loss: 20.0270\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.3758 - val_loss: 23.0972\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 33.8334 - val_loss: 26.5780\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 34.3335 - val_loss: 21.5920\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 39.5225\n",
            "test loss, test mse: 39.52249526977539\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 1s 12ms/step - loss: 72.9405 - val_loss: 76.7543\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 47.7604 - val_loss: 50.9705\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 35.6632 - val_loss: 41.4754\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 35.5914 - val_loss: 50.3170\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 38.0937 - val_loss: 45.0382\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.3854 - val_loss: 43.4766\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 31.4014 - val_loss: 42.0436\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 34.8914 - val_loss: 44.6471\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 36.4817 - val_loss: 43.5701\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.0324 - val_loss: 40.0786\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 34.1315\n",
            "test loss, test mse: 34.13148498535156\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 69.3224 - val_loss: 65.5864\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 40.0293 - val_loss: 34.0389\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 40.0331 - val_loss: 35.9548\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 32.7252 - val_loss: 33.6074\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 31.4431 - val_loss: 43.1052\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 33.9545 - val_loss: 32.7276\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.5414 - val_loss: 41.7991\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 31.2693 - val_loss: 34.7752\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 31.1582 - val_loss: 31.7439\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 29.7153 - val_loss: 33.4116\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 38.7102\n",
            "test loss, test mse: 38.71018600463867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpTIbUwF5EMN"
      },
      "source": [
        "이렇게 하면, CV를 통해서 모델을 돌릴 수 있는 것까지 확인해보았습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm7zow5IvaTt"
      },
      "source": [
        "### 입력 데이터 정규화 (Normalizing)\n",
        "\n",
        "  입력 데이터를 신경망에 넣기 전에 정규화나 스케일링이 무조건 해야하는 것은 아닙니다, 보통 신경망이 수치형 데이터를 받으면 자체적으로 적절한 가중치를 학습해서 데이터를 다룰 수 있기 때문입니다. 하지만 학습을 빠르게 해주고, 경사하강법이 지역 최적점(local optimum)에 빠질 위험을 줄여주기 때문에 하는 것을 추천합니다. [읽어볼거리](https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr0ykRBufdxA"
      },
      "source": [
        "# 정규화를 위한 함수 호출\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "print(x_train[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6hgCWbir90R"
      },
      "source": [
        "### 모델 자동 검증 기능\n",
        "\n",
        "우리가 늘 하는 train_test_split 대신, 케라스에는 `validation_data`라는 편리한 기능이 있습니다. 모델을 학습할 때 validation_data에 테스트 데이터를 입력하면 케라스에서 자동으로 테스트셋의 일정 부분을 검증용 데이터로 사용합니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMXVfmzXp1Oo"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 중요한 하이퍼 파라미터들\n",
        "inputs = x_train.shape[1]\n",
        "epochs = 75                 # 전체 반복횟수\n",
        "batch_size = 10             # 한번에 학습하는 사이즈\n",
        "\n",
        "\n",
        "# 모델을 생성합니다\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Sequential인 경우, 아래의 방법으로도 모델을 만들 수 있습니다.\n",
        "# model = Sequential(\n",
        "# [\n",
        "#     Dense(64, activation='relu', input_shape=(inputs,)),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dense(1)\n",
        "# ]\n",
        "# )\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
        "\n",
        "# Fit Model\n",
        "model.fit(x_train, y_train, \n",
        "          validation_data=(x_test,y_test),  # validation set\n",
        "          epochs=epochs,                    # 전체 반복횟수\n",
        "          batch_size=batch_size             # 한번에 학습하는 사이즈\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYJ8t_ezHP4W"
      },
      "source": [
        "### 하이퍼 파라미터 튜닝 방식\n",
        "\n",
        "#### 1) Babysitting AKA \"Grad Student Descent\". \"육아\", 혹은 대학원생 갈아넣기\n",
        "\n",
        "다윈의 진화론을 아시나요? '자연 선택'이란 단어가 진화를 주도했다고 말하는데요, 우리의 선택은 자연이 해주지 않습니다. 만약 이전의 프로젝트에서 성능을 높이기 위해서 하이퍼 파라미터를 수없이 조정해봤다면, 또는 어제 여러분이 그런 일을 해보셨다면, 바로 이것을 표현한 것입니다. 이 방법은 100% 수동으로 진행되고 있어요. 학계에서 논문을 출간할 수 있을 정도로 놀라운 정확도를 보여주는 그 하이퍼 파라미터의 수치를 찾아내기 위해 쓰는 방법이죠. 물론 지도교수님들이 이 걸 직접 하시진 않습니다, 교수님의 시간은 소중하니까요... 이 방법은 소위 노가다라고 불리고 \"제출 시간 다 될 때까지 조금이라도 하이퍼 파라미터를 더 건드려본다\" 방법으로도 알려져 있습니다.\n",
        "\n",
        "#### 2) Grid Search\n",
        "\n",
        "Grid Search는 대학원생이 \"그냥 내가 실험할 것들만 컴퓨터한테 알아서 모든 조합을 돌리게 하고 난 그동안 밥이나 먹고 오면 되는 거 아닌가\"라는 기가막힌 발상입니다. 하지만 Section 2에서 여러분도 이미 겪어봤겠지만 단점이 너무나 뚜렷합니다. 범위를 잘못 설정하면 여러분이 수료하고 취직을 하고나서도 끝나지 않을 가능성도 있다라는 것입니다. 가령 5개의 하이퍼 파라미터를 5개의 값으로 각각 조정하면 5^5 = 3125개의 다른 모델을 실험하게 하는 것인데 여기다 교차 검증으로 cv = 5를 하게되면 모델은 총 15,525번을 돌아야합니다. 보다시피 전수탐색 방법은 잘못 쓰면 끝나지 않는 루프속으로 빠져들지만, 잘 활용한다면 모델의 성능을 끌어올리는데 큰 도움이 됩니다.\n",
        "\n",
        "Grid Search를 사용할 계획이라면, 여러 하이퍼 파라미터의 조합을 찾는 데 사용하지 말아야합니다. Grid Search는 오직 하나의 파라미터의 최적값을 찾는데 활용하시길 바랍니다. 다른 하이퍼 파라미터의 조합으로 모델의 성능이 획기적으로 올라가는 경우는 상당히 드뭅니다. 모델을 위한 하이퍼 파라미터만 제대로 튜닝해서 최적값을 쓰면서 하나씩 하이퍼 파라미터들을 조정해가면 못해도 90~95% 정도 원했던 성능값을 얻을 수 있습니다. 이런 식으로 접근하면 적어도 위에서 언급한 무한루프가 생길 위험을 줄일 수 있습니다.\n",
        "\n",
        "#### 3) Random Search\n",
        "\n",
        "Grid Search를 몇시간정도 하다보면 불현듯 자신에게 질문을 하게 됩니다. \"더 나은 방법이 있을텐데...\" 맞습니다, RandomSearch가 그 답입니다. RandomSearch를 활용하면 지정된 범위내에서 무작위로 선정한 조건으로 모델을 돌려본 후, 이 구성이 그나마 나으니까 이거 쓰고 이제 집에 가서 가족들이랑 시간을 보내라고 할 것입니다. \n",
        "\n",
        "Grid Search는 모든 파라미터가 동등하게 중요하다고 전제를 합니다, 하지만 꼭 그렇다고 할 수는 없습니다. 파라미터에 따라서 생각보다 더 크게 범위를 건너 뛰어야 되는 경우도 있습니다. Random Search는 상대적으로 중요하다고 생각되는 파라미터에 대해 탐색을 더 하고, 덜 중요한 하이퍼파라미터에 대해서는 실험을 덜 할 수 있도록 해줍니다. Random Search의 단점은 절대적으로 완벽한 하이퍼 파라미터를 찾아주진 않습니다. 하지만 Grid Search와 비교하면 덜 시간을 소모한다는 장점만으로도 충분히 빛을 발할 수 있습니다.\n",
        "\n",
        "#### 4) Bayesian Methods\n",
        "\n",
        "\"육아\" 방식과 GridSearch와 같은 수동적인 방식을 더 효과적으로 만들어 줄 수 있는 방식은 실험자가 결과를 보고 추후 탐색에 그 결과에서 얻은 정보를 반영하는 것입니다. 베이지안 최적화 방식을 활용한다면 우리의 하이퍼 파라미터 튜닝 방식의 \"하이퍼 파라미터 튜닝\" 할 수 있습니다. 신경망은 최적화 문제를 위한 최적화 문제 같아서 베이지안 최적화는 이전 탐색 결과를 반영해서 이후의 하이퍼 파라미터 튜닝의 성능을 높이는 전략입니다. 베이지안 방식을 활용하고 싶다면 케라스의 `keras-tuner`를 쓰면 간단하게 하실 수 있습니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obzx6QgIfdxI"
      },
      "source": [
        "## 튜닝가능한 파라미터 옵션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfQ7D043OMMn"
      },
      "source": [
        "실험해볼 수 있는 하이퍼 파라미터의 종류는 다음과 같습니다:\n",
        "\n",
        "- batch_size\n",
        "- training epochs\n",
        "- optimization algorithms\n",
        "- learning rate \n",
        "- momentum\n",
        "- activation functions \n",
        "- dropout regularization \n",
        "- hidden layer의 neuron 갯수\n",
        "\n",
        "더 많은 하이퍼 파라미터가 존재하지만 중요한 것들을 골라보았고 일부는 반복해서 설명하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wefpKK_sBg__"
      },
      "source": [
        "# 데이터를 불러옵니다. \n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "# 정규화를 위한 함수 호출\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "print(x_train[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mri5-kXzVKAa"
      },
      "source": [
        "### Batch Size\n",
        "\n",
        "배치 크기는 모델이 경사하강법을 통해 손실/오차 계산을 해서 모델의 가중치를 업데이트할 때 **한번에 몇개의 관측치를 보게 되는지**를 결정하는 파라미터입니다. 우리가 찾고자 하는 가장 적절한 곳은 가중치를 업데이트 할 수 있을만큼의 충분한 정보를 제공할 수 있는 딱 충분한 양의 관측치들인데, 이 때 너무 큰 배치 크기를 고르게 되면 한번에 모든 데이터에 대한 Loss를 계산해야 하는 문제점이 있고, 학습 속도가 빠르기 때문에 주어진 epoch 안에 가중치를 충분히 업데이트 할 만큼의 iteration을 돌릴 수 없게 되기에 적당한 양을 고르는 것이 중요합니다. Feed-forward 신경망은 다른 구조에 비해 배치 크기의 영향을 크게 받지 않습니다만, 그래도 튜닝이 필요한 중요한 하이퍼 파라미터입니다. 만약 너무 작은 사이즈를 고른다면 학습에 오랜 시간이 걸리고, 추정값에 노이즈가 많이 생기기 때문에 이 역시 지양해야합니다. 작은 Batchsize를 잘 고르면, Generlization이 잘 된다는 보고도 있습니다. 그러나 [BatchNorm](https://en.wikipedia.org/wiki/Batch_normalization)을 사용하는 효과를 볼 수 없다는 [보고](http://proceedings.mlr.press/v89/lian19a/lian19a.pdf)가 있습니다. \n",
        "\n",
        "대체적으로 배치 사이즈는 32에서 시작해서 512에서 멈추는 2의 제곱수로 결정합니다. 케라스는 default로 32를 배치사이즈로 정하고 있습니다. 얀 레컨의 유명한 트윗 중 하나를 보여드리겠습니다.\n",
        "> 큰 미니배치로 학습하는 건 건강에 좋지않습니다. 주변에 혹시 32보다 큰 미니배치 쓰는 사람 있으면 돗자리 깔고 말리세요.\n",
        "\n",
        "저 트윗의 의미가 궁금하다면 다음의 [논문](https://arxiv.org/abs/1804.07612)을 참고해보시길 바랍니다.\n",
        "\n",
        "왜 배치 크기가 2의 제곱수로 설정되는지 궁금하다면 [다음 글](https://datascience.stackexchange.com/questions/20179/what-is-the-advantage-of-keeping-batch-size-a-power-of-2)을 읽어보시기 바랍니다\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2smXfriNAGn7"
      },
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# 재현성을 위해 랜덤시드를 생성합니다\n",
        "#seed = 7\n",
        "numpy.random.seed(1100)\n",
        "\n",
        "# 데이터셋을 불러옵니다.\n",
        "url =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "dataset = pd.read_csv(url, header=None).values\n",
        "\n",
        "# 불러온 데이터셋을 X와 Y로 나눕니다\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# 모델을 만들기 위한 함수를 만듭니다 (KerasClassifier 요구 사항)\n",
        "def create_model():\n",
        "    # 모델 제작\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # 모델 컴파일링\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# keras.wrapper를 활용하여 분류기를 만듭니다\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# GridSearch\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [30]\n",
        "param_grid = dict(batch_size=batch_size)\n",
        "\n",
        "# GridSearch CV를 만듭니다\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "# 최적의 결과값을 낸 파라미터를 출력합니다\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKcuY6OiaLfz"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "**최적화도구(Optimizer, 옵티마이저)**에 지난 시간에 많이 다뤄보았기 때문에 대해서는 간단하게 언급하고 넘어가겠습니다. 시간이 된다면 케라스에 있는 다양한 옵티마이저들에 대해 읽어보시길 바랍니다. \"adam\" optimizer가 보통 제일 좋은 결과를 제공합니다. 요즘은 adamW 등도 많이 사용하는데, 무엇을 선택하든 옵티마이저를 선택하는데 있어서 알아야 할 것은 옵티마이저에 따라 하이퍼 파라미터의 종류, 값 역시 달라진다는 점입니다 (learning rate, momentum, etc.) 그래서 여러분이 어떤 옵티마이저를 선택하느냐에 따라서 옵티마이저의 learning rate이나 momentum을 튜닝해야 할 수도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3wq5iOaLig"
      },
      "source": [
        "### Learning Rate\n",
        "\n",
        "  Learning Rate은 경사하강법 기반의 optimizer 선택을 위한 하이퍼 파라미터입니다. 그러면 기존에는 어떻게 사용되었을까요? 기본 값으로 0.01로 [설정](https://www.google.com/search?q=keras+default+learning+rate&oq=keras+default+learning+rate&aqs=chrome..69i57j0i22i30l2.4191j0j7&sourceid=chrome&ie=UTF-8)되어있습니다. learning rate (학습율)이 너무 높은 경우에는 모델이 발산하게 해버리는데, 반대로 너무 낮게 설정하면 모델이 수렴하는데 실패하게 됩니다 (최적의 학습율을 찾아야 하는거죠). 일단은 크기순으로 learning rate을 튜닝합니다 ([.001, .01, .1, .2, .3, .5]). 0.5보다 높이 잡는 것은 추천하지 않지만, 한번 높이 설정한 후에 그 이유를 알아보는 것도 좋은 공부가 될 것입니다.\n",
        "\n",
        "어느정도 범위를 좁혔다면, 갯수를 줄여서 다시 한번 해봅니다. 만약 시도한 후에 모델이 0.1을 최적의 옵티마이저라고 한다면,[.05, .08, .1, .12, .15]로 시도해서 더 좁혀들어가보면 좋을 것 같습니다 (아직 \"최고\"를 찾진 못했으니까요) \n",
        "\n",
        "학습율의 튜닝과 같이 epoch의 횟수도 튜닝하는 것이 좋습니다. 왜냐하면 learning rate이 최솟값에 도달할 수 있을 때까지 iteration의 횟수를 충분히 설정했는지를 판단할 수 있기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNTBUWd1aLlA"
      },
      "source": [
        "### Momentum\n",
        "\n",
        "모멘텀은 기본 **확률적 경사 하강법(SGD)과 대체적으로 많이 연관**됩니다. SGD는 꽤 흔한 옵티마이저인데요, 그 이유는 이해하기 쉽고, 교재에서 대표적으로 다루고 있기 때문입니다. 그만큼 사람들이 많이 이해하고 알고 있는 옵티마이저라는 것이죠. SGD를 이용해서 최고의 결과값을 도출할 수 있을까요? 아마 어려울 것입니다. 가장 단순한 형태이기 때문이죠. 만약 하이퍼 파라미터 튜닝을 통해서 속성들을 튜닝했을 때, 우리가 대표적으로 사용하는 adam(Adaptive Moment estimation) 옵티마이저보다 좋은 결과를 낼 수 있는지 도전해봐도 좋을 것 같습니다. \n",
        "  모멘텀은 옵티마이저가 최솟값을 overshooting 하게 결정하는 속성입니다. 오목한 밥그릇 한쪽에서 공을 굴리면 반대쪽에 있다가 바로 밑으로 내려가지않고 관성에 의해서 잠시 머물다 다시 내려오는 것을 상상해보세요. 모멘텀의 목적은 지역 최소점(local minima)에서 탈출하도록 시도하는 것입니다\n",
        "\n",
        "Adam이 나오기까지의 옵티마이저의 변천을 잘 정리한 [블로그 글](https://sacko.tistory.com/42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oul9sPq-dU-h"
      },
      "source": [
        "### Network Weight Initialization\n",
        "\n",
        "우리는 처음에 망의 가중치 초기화를 어떻게 하는지에 따라 결과에 얼마나 큰 영향을 끼치는지 봤습니다. 가중치 초기화 모드는 매우 다양합니다 (아래에 종류를 적어놨습니다). 모든 것을 써볼 일은 아마 없겠지만 어떤 것을 선택하는지에 따라 모델의 초기 정확도에 큰 영향을 끼칩니다. 우리가 가중치를 잘 초기화하면 훨씬 적은 epoch으로 모델을 위한 최적의 가중치를 찾을 수 있습니다.\n",
        "\n",
        "> 가중치 표준편차를 1인 정규분포로 초기화를 할 때 활성화 값의 분포\n",
        "\n",
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/994C2F3C5AB623C526\" width=600/>\n",
        "\n",
        "> 가중치의 편차를 1/sqrt(n) 으로 초기화한 Xavier 초기값의 활성화 값의 분포\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbuwPPz%2FbtquO7Wq9Rp%2Fylz2Qsc0fi9m0TaQNXBYDK%2Fimg.png\" width=580/>\n",
        "\n",
        "> 가중치 표준편차를 sqrt(2/n)으로 초기화한 He 초기값의 활성화 값의 분포\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcKBoWH%2FbtquO7B8MfF%2FMs5LyROpCV89EbCFNXja4k%2Fimg.png\" width=580/>\n",
        "\n",
        "c.f. Activation function에 따른 초기값 추천\n",
        "   > ① Sigmoid  ⇒  Xavier 초기화를 사용하는 것이 유리 \n",
        "\n",
        "   > ② ReLU  ⇒  He 초기화 사용하는 것이 유리\n",
        "\n",
        "\n",
        "`init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnEG-bCJaLnZ"
      },
      "source": [
        "### (복습) Activation Functions\n",
        "\n",
        "활성화 함수에 대해선 잠깐 이야기를 나눴는데요, 보통은 은닉층에는 ReLU를 사용하고 출력층에는 Sigmoid (이진 분류)나 Softmax (다중분류)를 사용합니다, 하지만 모델에 따라서 `sigmoid`, `tanh` 등 다른 활성함수들을 시도해보고 결과가 더 괜찮게 나오는지 확인해보는 것도 좋습니다 (물론 역시는 역시나일 수도 있습니다).   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqtEuxeQaLqE"
      },
      "source": [
        "### (복습) 드롭아웃과 가중치 규제 (Weight Constraint)\n",
        "\n",
        "드롭아웃 규제값은 학습 중에 무작위로 비활성화 하고 싶은 뉴런들의 비율입니다. 지난 시간에 공부했던 것처럼, 가중치 제약(Weight Constraint)은 드롭아웃과 함께 쓰는 두번째 규제 파라미터입니다. 튜닝을 할 때는 이 두개값 모두 튜닝해야합니다.\n",
        "드롭아웃을 어느 layer (visible vs. hidden)에 적용하는지에 따라 다른 효과를 불러올 수 있습니다. 은닉층에 드롭아웃을 썼을 때 엄청난 효과를 부를 때도 있지만, 반대로 아무런 일도 일어나지 않을 수 있는 것 입니다. 사용하고 있는 모델이 과적합이나 일반화 문제가 있지 않다면 굳이 쓸 필요는 없습니다. \n",
        "다음 주에 공부하게될 다양한 모델들을 보시면, Dropout이 많이 적용되어 있는 것을 보실 수 있는데요, 그때에 아 여기에서는 규제방법들을 이용해서 문제를 해결해야 했구나 정도로 알고 넘어가시면 되겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2c5Cv6oaLtO"
      },
      "source": [
        "### (복습) 은닉층의 뉴런들의 수\n",
        "\n",
        "\n",
        "우리가 하나의 퍼셉트론만 있었을 때 선형적으로 분리가 가능한 데이터만 학습이 가능했던 것을 기억하시나요? 하지만 layer와 노드들을 신경망에 추가할 수록 비선형 데이터 학습도 가능하게 되었습니다. 신경망이 크면 클수록, 노드가 많으면 많을 수록 신경망 역시 비선형 데이터도 학습 시킬 수 있습니다. 다만 많은 노드와 레이어는 학습 시간을 늘리게 되고 모델이 과적합할 확률을 높입니다. 신경망이 커질수록 드롭아웃 규제나 다른 규제 방법으로 이러한 가능성들에 대비하고 있어야 합니다.\n",
        "\n",
        "보통 깊이 (layer 추가)가 길이 (노드 추가)보다 신경망에 더 중요합니다. 이것이 딥러닝에 사람들이 몰리는 이유입니다. 몇가지 딥러닝 구조는 몇가지 머신러닝 프로젝트들에 혁신을 불러왔습니다.\n",
        "다른 망 구조를 참조하게 될 수도 있습니다. 예를 들어 이미지 분류 문제를 풀 때 만약 resnet, inception, ResNeXt, SENet, EfficientNet 등의 SOTA (State of the Art: 최신/최고 기술)를 활용하지 않았다고 가정해보겠습니다. 그럼 아마도 좋은 성능의 모델을 찾기 위해서 수많은 시간을 실험에 할애하게 될 것입니다.\n",
        "물론 휴리스틱(사람이 직접 설정한 값)들로도 좋은 결과를 낼 수 없을 지도 모릅니다. 그러나 학습기간에는 직접 다양한 값들을 실험해보면서 자신만의 감을 찾으면서 문제를 해결하는 것이 장기적으로 훨씬 더 많은 도움이 되기 때문에 아래의 글은 어디까지나 \"참고\"만 하시길 바랍니다. \n",
        "- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Keras Tuner 를 사용한 파라미터 튜닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "\n",
        "Keras Tunner는 TensorFlow 프로그램에 대한 최적의 하이퍼 파라미터 세트를 선택하는 데 도움이 되는 라이브러리입니다. 머신러닝(ML) 모델에 적합한 하이퍼 파라미터 세트를 선택하는 프로세스를 **하이퍼 파라미터 튜닝** 또는 **하이퍼 튜닝**이라고 부릅니다.\n",
        "\n",
        "하이퍼 파라미터는 ML 모델의 학습 프로세스와 토폴로지를 좌우하는 변수입니다. 이러한 변수는 학습 프로세스에 걸쳐 일정하게 유지되며 ML 모델의 성능에 직접적인 영향을 미칩니다. 하이퍼 파라미터는 두 가지 유형입니다.\n",
        "1. **Hidden Layer의 수 및 노드 수**와 같은 모델 선택에 영향을 미치는 **모델**과 관련된 하이퍼 파라미터입니다.\n",
        "2. 확률적 경사 하강법(SGD) **학습률** 및 kNN(가장 가까운 이웃) 분류자의 **가장 가까운 이웃 수** 등 학습 알고리즘의 속도와 **품질**에 영향을 미치는 파라미터입니다.\n",
        "\n",
        "Keras Tuner를 사용하여 이미지 분류 모델에 대한 하이퍼 튜닝을 수행해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83Lwsy-Aq2_"
      },
      "source": [
        "기본적으로 설치가 안되어있는 모듈이기 때문에 설치를 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_leAIdFKAxAD"
      },
      "source": [
        "!pip install -U keras-tuner\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReV_UXOgCZvx"
      },
      "source": [
        "## Fashion MNIST 데이터\n",
        "\n",
        "[Fashion MNIST 데이터 세트](https://github.com/zalandoresearch/fashion-mnist)를 사용하여 이미지를 분류하는 기계 학습 모델에 가장 적합한 하이퍼 파라미터 튜닝을 해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHlHs9Wj_PUM"
      },
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLVhXs3xrUD0"
      },
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YEL2H2Ax3e"
      },
      "source": [
        "## model 구조 제작\n",
        "\n",
        "하이퍼 튜닝 모델을 작성할 때 모델 아키텍처 외에 하이퍼 파라미터 `검색 공간`도 정의할 수 있습니다.\n",
        "\n",
        "사용할 두 가지 방법:\n",
        "* model builder function을 사용\n",
        "* Keras Tuner API의 `HyperModel` 클래스를 하위 분류기를 사용\n",
        "\n",
        "컴퓨터 비전 응용 프로그램에 [HyperXception](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperxception-class) 및 [HyperResNet](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperresnet-class)이라는 두 가지 미리 정의된 HyperModel 클래스를 사용할 수도 있습니다.\n",
        "\n",
        "model builder function을 사용하여 이미지 분류 모델을 만들고, model builder는 컴파일된 모델을 반환하고 사용자가 정의한 하이퍼 파라미터를 사용하여 모델을 하이퍼튠합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQKodC-jtsva"
      },
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(Flatten(input_shape=(28, 28)))\n",
        "  \n",
        "  # 첫 번째 Dense layer에서 노드 수를 조정(32-512)합니다.\n",
        "  hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
        "  model.add(Dense(units = hp_units, activation = 'relu'))\n",
        "  model.add(Dense(10))\n",
        "\n",
        "  # Optimizer의 학습률(learning rate)을 조정[0.01, 0.001, 0.0001]합니다. \n",
        "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
        "  \n",
        "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
        "                loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1VYw4q3x0b"
      },
      "source": [
        "## 튜너를 인스턴스화하고 하이퍼 튜닝을 수행\n",
        "\n",
        "튜너를 인스턴스화하여 하이퍼 튜닝을 수행합니다. 캐러스 튜너에는 - `RandomSearch`, `Hyperband`, `BayesianOptimization`, 그리고 `Sklearn`. 이 튜토리얼에서는 [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) 튜너를 사용합니다.\n",
        "\n",
        "Hyperband 튜너를 인스턴스화하려면 하이퍼 모델, 최적화할 `objective` 및 훈련할 최대 epochs 수('max_epochs')를 지정해야 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oichQFly6Y46"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective = 'val_accuracy', \n",
        "                     max_epochs = 10, # 일반적으로 10, 빠른 구현을 위해서 숫자를 줄였음.\n",
        "                     factor = 3,\n",
        "                     directory = 'my_dir',\n",
        "                     project_name = 'intro_to_kt')                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIhhdKf9VtI"
      },
      "source": [
        "\n",
        "Hyperband tuning 알고리즘은 적응형 리소스 할당 및 early-stopping 기능을 사용하여 고성능 모델을 신속하게 통합합니다. 이 작업은 스포츠 챔피언 스타일 브래킷을 사용하여 수행됩니다. 이 알고리즘은 몇 년 동안 많은 수의 모델을 교육하고 최고 성능의 절반만 다음 라운드로 전달합니다. 하이퍼밴드는 1 + log<sub>`factor`</sub>(`max_epochs`)를 계산하여 가장 가까운 정수로 반올림하여 브래킷에서 교육할 모델 수를 결정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQYevi4JfsK2"
      },
      "source": [
        "## Callback 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTeamrUWOJRb"
      },
      "source": [
        "하이퍼 파라미터 검색을 실행하기 전에 모든 교육 단계가 끝날 때마다 학습 출력을 지우도록 콜백을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbr_8QE76PJd"
      },
      "source": [
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSBQcTHF9cKt"
      },
      "source": [
        "tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "하이퍼 파라미터 검색이 완료되었습니다. \n",
        "최적화된 첫 번째 Dense 노드 수는 {best_hps.get('units')} 입니다.\n",
        "최적의 학습 속도는 {best_hps.get('learning_rate')} 입니다.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lak_ylf88xBv"
      },
      "source": [
        "사용하기 전에는 최적의 하이퍼 파라미터로 모델을 다시 학습한 뒤 사용하면 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McO82AXOuxXh"
      },
      "source": [
        "# 최적의 하이퍼 파라미터를 사용하여 모델을 구축하고 데이터에 대해 교육\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLFxtmQPclEU"
      },
      "source": [
        "model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQRpPHZsz-eC"
      },
      "source": [
        "`my_dir/intro_to_kt` 디렉토리에는 하이퍼 파라미터 검색 중에 실행되는 모든 평가판(모델 구성)에 대한 세부 로그 및 체크포인트가 포함되어 있습니다. 하이퍼 파라미터 검색을 다시 실행하는 경우 Keras Tuner는 이러한 로그의 기존 상태를 사용하여 검색을 다시 시작합니다. 이 동작을 비활성화하려면 튜너를 인스턴스화하는 동안 추가 `overwrite = True` 인수를 전달합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKwLOzKpFGAj"
      },
      "source": [
        "## 튜너에 대해서 더 알아보기\n",
        "  Keras Tuner를 사용하여 모델의 하이퍼 파라미터를 조정하는 방법에 대해 배웠습니다. Keras Tuner에 대한 자세한 내용은 다음 추가 리소스를 참조하십시오.\n",
        "\n",
        "* [Keras Tuner 텐서플로우 블로그](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n",
        "* [Keras Tuner 웹사이트](https://keras-team.github.io/keras-tuner/)\n",
        "\n",
        "또한 [하이퍼파라미터 튜닝 대시보드](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams)를 체크해 보시고, 모델을 만들 때 참고하면 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XVBDUE2fdxQ"
      },
      "source": [
        "# 실험 기록 프레임워크\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX7aetDxfdxQ"
      },
      "source": [
        "## 왜 실험기록을 합니까?\n",
        "\n",
        "다양한 파라미터를 변경하는 실험을 진행하다보면 점점 결과들을 관리하는 것이 점점 힘들어지는 것을 느끼게 될 것입니다. 어떤 파라미터 조합이 제일 좋았지? 어제 했던 결과와 차이가 있었던가? 비록 파이썬 노트북을 활용하고 있지만, 사실 실험 결과를 기록하기엔 적절하지 않습니다. 이 때 Comet.ml 과 Weights and Biases가 이러한 고민들을 해결 해줄 수 있습니다.\n",
        "이 도구들은 여러분이 실험을 실시간으로 기록하고, 실험에 쓰인 코드와 결과값을 보관해줍니다. 실험 결과는 원하는 평가지표로 언제든지 시각화해서 모델의 성능을 볼 수 있게 해줍니다. epoch이 끝날 때마다 데이터가 해당 툴들에 보내지고, 모델이 수렴하고 있는지 확인할 수 있습니다. 오늘은 Weights and Biases를 활용해보겠습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZCfTFuKfdxR"
      },
      "source": [
        "## Wandb 이용 - 설치 및 회원가입 필요 \n",
        " \n",
        "먼저 다음 셀을 실행하기 전에 터미널에서 `wandb`에 로그인이 되있어야 합니다. \n",
        "\n",
        "```zsh\n",
        "# 아래의 커맨드를 실행합니다\n",
        "wandb.login\n",
        "```\n",
        "구체적인 방법은 Weights and Biases의 [QuickStart](https://docs.wandb.com/quickstart)를 참고해주시길 바랍니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnsBwJ-ToZMK"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wpf-SzxfdxS"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RWmT0WCfdxU"
      },
      "source": [
        "# group, project 변수를 설정합니다. 반복되는 이름이 많기 때문에 변수로 설정하여 사용하면 편리합니다.\n",
        "wandb_project = \"review\"\n",
        "wandb_group = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTHKXKK3pMsh"
      },
      "source": [
        "# !git clone http://github.com/wandb/tutorial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68WPyCBypMvS"
      },
      "source": [
        "# !cd tutorial; pip install --upgrade -r requirements.txt;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3-zYyZUpMyS"
      },
      "source": [
        "!wandb login 6a1f7dd199ef2c241cc2dafb7ae52925d6de7385"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtmJPO-tfdxW"
      },
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.python import keras\n",
        "#from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsK6V_Mt1P1Y"
      },
      "source": [
        "# !python -c \"import keras; print(keras.__version__)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NuY-S7-08AB"
      },
      "source": [
        "wandb.init(project=wandb_project)  ## 내가 만든 프로젝트 이름을 넣어주어야 합니다.\n",
        "#wandb.init(project=wandb_project, entity=wand_group) \n",
        "\n",
        "# 데이터 및 하이퍼파라미터 설정 \n",
        "X =  x_train\n",
        "y =  y_train\n",
        "\n",
        "inputs = X.shape[1]\n",
        "wandb.config.epochs = 50\n",
        "wandb.config.batch_size = 10\n",
        "\n",
        "# 모델을 구축합니다\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "# 모델을 컴파일 합니다\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
        "\n",
        "# 모델을 학습합니다\n",
        "model.fit(X, y, \n",
        "          validation_split=0.3, \n",
        "          epochs=wandb.config.epochs, \n",
        "          batch_size=wandb.config.batch_size, \n",
        "          callbacks=[WandbCallback()]\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKAePdyn8d3i"
      },
      "source": [
        "wandb.init(project=wandb_project)  ## 내가 만든 프로젝트 이름을 넣어주어야 합니다.\n",
        "\n",
        "# 데이터 및 하이퍼파라미터 설정 \n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "wandb.config.epochs = 10\n",
        "wandb.config.batch_size = 64\n",
        "\n",
        "# 모델을 구축합니다\n",
        "model = Sequential() ## 과제시에는 이 모델을 Tre-trained model로 대체하면 됩니다. \n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbDi0Zuqt4AP"
      },
      "source": [
        "# 모델학습방식을 정의함\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습시키기\n",
        "model.fit(train_images, train_labels, \n",
        "          validation_data=(test_images, test_labels),\n",
        "          epochs=wandb.config.epochs, \n",
        "          batch_size=wandb.config.batch_size, \n",
        "          callbacks=[WandbCallback()]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzhEZt4FCl6U"
      },
      "source": [
        "성능이 마음에 안든다면 추가로 학습을 더 시키는 방법도 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FU56rB8CQoU"
      },
      "source": [
        "wandb.config.epochs = 20\n",
        "wandb.config.batch_size = 512\n",
        "\n",
        "model.fit(train_images, train_labels, \n",
        "          validation_data=(test_images, test_labels),\n",
        "          epochs=wandb.config.epochs, \n",
        "          batch_size=wandb.config.batch_size, \n",
        "          callbacks=[WandbCallback()]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Md_CV8E9Olj"
      },
      "source": [
        "!ls wandb/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-9xS_PM7Q2_"
      },
      "source": [
        "다음으로는 프로그램 웹 페이지로 접속해서 분석해보는 시간을 가집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvN7h5Sfdxt"
      },
      "source": [
        "# Review\n",
        "* <a href=\"#p1\">Part 1</a>: 각각에 대해서 설명할 수 있는 지, 이해되지 않은 것들은 없는지 확인해 봅시다.\n",
        "    - Activation Functions\n",
        "    - Optimizer\n",
        "    - Number of Layers\n",
        "    - Number of Neurons\n",
        "    - Batch Size\n",
        "    - Dropout\n",
        "    - Learning Rate\n",
        "    - Number of Epochs\n",
        "    - and many more\n",
        "* <a href=\"#p2\">Part 2</a>: 실험 추적 프레임워크를 구현을 스스로 해볼 수 있는 지 점검합시다\n",
        "    - Weights & Biases\n",
        "    - Comet.ml\n",
        "    - By Hand / GridSearch\n",
        "* <a href=\"#p3\">Part 3</a>: 케라스 튜너를 이용하여 최적의 파라미터를 찾는 방법을 점검합시다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COUqF7gQfdxt"
      },
      "source": [
        "## 읽어볼만한 자료\n",
        "- [Grid Search Hyperparameters for Deep Learning](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)\n",
        "- [Hyperparameters Optimization for Deep Learning Models](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/)\n",
        "- [Dropout Regularization in Deep Learning](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)\n",
        "- [Weight Constraints in Deep Learning](https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/)\n",
        "- [Number of Layers and Nodes in a Neural Network](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)\n",
        "- [Batch Normalization](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31MSAl6eqWvz",
        "outputId": "7f8c1dd8-e7fa-41a3-ccb5-9dafde251169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGqNcZ_YhsG0"
      },
      "source": [
        "!pip freeze > \"drive/MyDrive/Colab Notebooks/section4/N414_requirements.txt\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2ECIMO9qPde",
        "outputId": "7dbeb692-6d06-4776-91a3-ae65a7cb1266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls drive/MyDrive/Colab\\ Notebooks/section4"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 01_My_First_Deeplearning.ipynb\n",
            " AI-history-new.jpg\n",
            " Comet-pytorch.ipynb\n",
            " ds-section4-NN-classification.ipynb\n",
            " DS-Unit-4-Sprint-1-NLP-master\n",
            " environment.yml\n",
            " 셀프체크리스트.gdoc\n",
            " Lambda_N42\n",
            " Lambda-N43\n",
            " LS_DS_421_Intro_to_NN_Assignment.ipynb\n",
            " LS_DS_423_Keras_Lecture.ipynb\n",
            " N411_requirements.txt\n",
            " N412_requirements.txt\n",
            " N413_requirements.txt\n",
            " N414_requirements.txt\n",
            " n421a-artificial-neural-networks.ipynb\n",
            " n421a-sol-artificial-neural-networks.ipynb\n",
            " N431_requirements.txt\n",
            " N432_requirements.txt\n",
            " N433_requirements.txt\n",
            " N434_1_requirements.txt\n",
            " SC\n",
            "'sprint1(Lee)'\n",
            " sprint2\n",
            " sprint3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}